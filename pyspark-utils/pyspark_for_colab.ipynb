{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark-for-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ8Nvjqcu+BhktbfJ1rg8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domvwt/uol-ds-tools/blob/main/pyspark-utils/pyspark_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxDhDCl2QbL"
      },
      "source": [
        "# PySpark Setup for Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8bh-cH72UjB"
      },
      "source": [
        "## Prepare Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uG_oCvBCaUJ",
        "outputId": "199f99bc-016b-4bcd-aac2-14316a04e8d1"
      },
      "source": [
        "%%shell\n",
        "\n",
        "SPARK_VERSION=\"3.1.1\"\n",
        "HADOOP_VERSION=\"3.2\"\n",
        "\n",
        "echo \"Preparing Spark requirements...\"\n",
        "if ! test -d \"spark\"; then\n",
        "  echo \"Updating system...\" && \\\n",
        "  sudo apt-get -yqq update && 2>&1 > /dev/null \\\n",
        "  sudo apt-get -yqq install openjdk-8-jre 2>&1 > /dev/null && \\\n",
        "  echo \"Downloading Spark...\" && \\\n",
        "  wget --quiet https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n",
        "  tar -xf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n",
        "  echo \"Installing Python libs...\" && \\\n",
        "  pip install -Uqq pyspark==${SPARK_VERSION} findspark && \\\n",
        "  mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark && \\\n",
        "  rm -rf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n",
        "  echo \"Complete.\"\n",
        "else\n",
        "  echo \"Requirements already satisfied.\"\n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing Spark requirements...\n",
            "Updating system...\n",
            "Downloading Spark...\n",
            "Installing Python libs...\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 63kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 43.5MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Complete.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bznVV18i2aUM"
      },
      "source": [
        "## Python Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj2wMKVeu8hO"
      },
      "source": [
        "import os\n",
        "import pyspark\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark\"\n",
        "findspark.init()\n",
        "try:\n",
        "  sc = pyspark.SparkContext().getOrCreate()\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbVWZEs11URP"
      },
      "source": [
        "## Test PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJjdOXIV1Zf0",
        "outputId": "a70b68d0-2100-431e-ce6e-77c9a2d5ff6d"
      },
      "source": [
        "# Download test file\n",
        "!wget https://www.gutenberg.org/files/84/84-0.txt -O frankenstein.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-22 14:02:31--  https://www.gutenberg.org/files/84/84-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 448821 (438K) [text/plain]\n",
            "Saving to: ‘frankenstein.txt’\n",
            "\n",
            "frankenstein.txt    100%[===================>] 438.30K  1.27MB/s    in 0.3s    \n",
            "\n",
            "2021-04-22 14:02:32 (1.27 MB/s) - ‘frankenstein.txt’ saved [448821/448821]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqVR0fna1a3E",
        "outputId": "6b9bbccf-1d47-41fa-8f8d-a5c4807e2980"
      },
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "INPUT_TXT = \"frankenstein.txt\"\n",
        "\n",
        "\n",
        "myfile = Path(INPUT_TXT).absolute()\n",
        "rdd_txt = sc.textFile(f\"file:///{myfile}\")\n",
        "\n",
        "# Simple word counts splitting on whitespace\n",
        "counts = (\n",
        "    rdd_txt.flatMap(lambda line: line.split())\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .map(lambda a: (a[1], a[0]))\n",
        ")\n",
        "\n",
        "res1 = counts.collect()[:20]\n",
        "for i in res1:\n",
        "    print(i)\n",
        "print()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(268, 'The')\n",
            "(79, 'Project')\n",
            "(2746, 'of')\n",
            "(3, 'Mary')\n",
            "(3, 'Wollstonecraft')\n",
            "(3, '(Godwin)')\n",
            "(3, 'Shelley')\n",
            "(318, 'is')\n",
            "(18, 'use')\n",
            "(7, 'anyone')\n",
            "(2, 'anywhere')\n",
            "(1129, 'in')\n",
            "(15, 'United')\n",
            "(7, 'States')\n",
            "(85, 'other')\n",
            "(25, 'world')\n",
            "(302, 'at')\n",
            "(154, 'no')\n",
            "(2, 'restrictions')\n",
            "(2, 'whatsoever.')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6M219TE1p9h"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}